train/dev/test sets:

- training set (98%)
- hold-out cross validation set / dev set (compare algorithms) (1%)
- test set (1%)

mismatched train/test distribution:

- make sure dev and test come from same distribution.

not having a test set might be okay. (only dev set)

~~~

bias / variance:

- high bias: underfitting
- high variance: overfitting


classification:
- high variance:  low train set error, high dev set error
- high bias: high train set error, high dev set error
- high bias and high variance: high train set error, higher dev set error
- low bias and high variance: low train set error, low dev set error

(assuming bayesian optimal error = 0%)

~~~

basic recipe for ml:


- is it high bias?  ->  bigger network, train longer, nn arch search
  (training data performance)

- high variance?    ->  more data, regularization, nn arch search
  (dev set performance)


no more "bias variance" tradeoff in deep learning.
training a bigger network almost never hurts.


~~~

regularization:

add a value to cost function J.

l2 regularization: (lambda / 2*m) * ||w||2**2  ----   used more often
l1 regularization: (lambda / 2*m) * ||w||1     ----   w will be sparse
lambda = regularization parameter  ----  set this using dev set

called frobenius norm instead of l2 norm.

dw = (from backprop) + (lambda/m) * w  ----  "weight decay"


~~~

why does regularization reduce overfitting?

- a lot of hidden units' weight reduces.
- this makes the network "simpler" as if you have removed those units.
- so you go from high variance to high bias, hopefully you stop in between.


~~~


